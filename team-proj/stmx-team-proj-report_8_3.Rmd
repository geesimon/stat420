---
title: "Residential Housing Price Prediction"
author: "STAT 420, Summer 2017, Martynas Sapoka (netid:msapoka2), Shailender Singh (netid: ss49), Tesa Ho (netid: tnho2), Xiaoming Ji (netid: xj9)"
netid: ""
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
---

#Introduction
##Overview
In this project, we will build a linear model that can predict residential home prices in Ames, Iowa based on several explanatory variables.  This project is a practical example of using real world data that consists of a mix of different data types - nominal, ordinal, discrete, and continuous variables. 

Our final project will incorporate the following topics covered in this course: 

- Variable manipulation
- Outlier identification
- Data analysis and interpretation
- Model building
- Model evaluation

##Description of Dataset

The data set describes the sale of individual residential property in Ames, Iowa from 2006 to 2010 and contains 2,919 observations.  There are 80 explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) involved in assessing home values.

The 23 nominal variables identify various types of dwellings, materials, garages, and environmental conditions.

The 23 ordinal variables range from 2 to 28 representing Streets (gravel or paved) and Neighborhoods (areas within the Ames city limits).

The 14 discrete variables quantify the number of items for each house.  Items include the number of kitchens, bedrooms, bathrooms, garage spaces and their respective location if the house has more than one floor. 

The 20 continuous variables are related to area dimensions for each house.  Variables include total dwelling square footage, total lot size, living area, and room dimensions. 

Some important variables that may have strong correlation with sale price.

- `GrLivArea`: Above grade (ground) living area square feet.
- `Neighborhood`: Physical locations within Ames city limits.
- `MSZoning`: Identifies the general zoning classification of the sale.
- `LotArea`:  Lot size in square feet.
- `OverallQual`: Overall material and finish quality.
- `TotalBsmtSF`: Total square feet of basement area.
- `YearBuilt`: Original construction date.
- `BedroomAbvGr`: Bedrooms above grade (does NOT include basement bedrooms).
- `BsmtFullBath`:  Full bathrooms above grade.
- `GarageCars`: Size of garage in car capacity.

##Dataset Source

Inspired by [`Kaggle Competition`](https://www.kaggle.com/c/house-prices-advanced-regression-techniques), the data set is provided by Dean De Cock from Truman State University.  The raw data comes directly from the Iowa State Assessorʻs Office. The initial Excel file contained 113 variables describing 3,970 property sales that had occurred in Ames, Iowa between 2006 and 2010. 
The variables were a mix of nominal, ordinal, continuous, and discrete variables used in calculation of assessed values and included physical property measurements in addition to computation variables used in the city’s assessment process. Variables that required specific housing or assessing knowledge or previous calculations were removed from the final dataset.  

# Methods
## Check the data
```{r, include=FALSE}
source("util.R")
```

We load the data and store it to variable `house_data`. Let's take a look on the data
```{r, include=FALSE}
library(readr)
house_data = read_csv("HousePrices/train_updated.csv")
head(house_data)
```

```{r, echo=FALSE, fig.height=8, fig.width=8}
par(mfrow=c(3,2))
plot(SalePrice ~ GrLivArea, data = house_data, pch  = 20, cex  = 2, col  = "dodgerblue")
hist(house_data$SalePrice, col = "dodgerblue",border = "darkorange")
hist(house_data$GrLivArea, col = "dodgerblue",border = "darkorange")
hist(house_data$LotArea, col = "dodgerblue",border = "darkorange")
boxplot(SalePrice ~ OverallQual, data = house_data, pch = 20, cex = 2, col = "darkorange", border = "dodgerblue")
boxplot(SalePrice ~ Neighborhood, data = house_data, pch = 20, cex = 2, col = "darkorange", border = "dodgerblue")
```

## Data Preprocessing
####Change Predictor Name
R doesn't allow variable name start with a number, thus we change some column names to comply with R variable definition.
```{r}
replace_names = function(data, name_pairs) {
  new_names = colnames(data)
  for (i in 1:length(name_pairs)) {
    new_names[which(new_names ==  name_pairs[i])] = names(name_pairs[i])
  }
  new_names
}

BadNames =  c(FirstFlrSF = "1stFlrSF", SecondFlrSF = "2ndFlrSF", ThreeSsnPorch = "3SsnPorch")

colnames(house_data) = replace_names(house_data, BadNames)
```

We stores all categorical predictor names in variable `CategoricalPredictors` and numerical predictor names in variable `NumericPredictors`.
```{r, include=FALSE}
CategoricalPredictors = c("MSSubClass", "MSZoning", "Street", "Alley", "LotShape",
            "LandContour", "Utilities", "LotConfig", "LandSlope", 
            "Neighborhood", "Condition1", "Condition2", "BldgType", 
            "HouseStyle", "OverallQual", "OverallCond", "RoofStyle", 
            "RoofMatl", "Exterior1st", "Exterior2nd", "MasVnrType",
            "ExterQual", "ExterCond", "Foundation", "BsmtQual", 
            "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", 
            "Heating", "HeatingQC", "CentralAir", "Electrical", 
            "KitchenQual", "Functional", "FireplaceQu", 
            "GarageType","GarageFinish", "GarageQual", "GarageCond", 
            "PavedDrive", "PoolQC", "Fence", "MiscFeature", 
            "SaleType", "SaleCondition", "MoSold")

NumericPredictors = colnames(house_data)[!colnames(house_data) %in% c(CategoricalPredictors, "Id")]
```
```{r}
(CategoricalPredictors)
(NumericPredictors)
```

####Missing Data
- Remove predictor with too many NAs (> 20%). In our data, NA is not a invalid value for most categorical predictors, but too many NA makes this predictor not very useful.
- Replace numerical NA with mean
- Convert categorical predictor to factor and make NA an extra value

```{r}
RM_Names = c()
for(name in colnames(house_data)) {
  nas = is.na(house_data[[name]])
  
  if(sum(nas) / nrow(house_data) > 0.2) {
    print(paste("(", name, ") Removed"))
    RM_Names = c(RM_Names, name)
    
  } else {
    if(name %in% CategoricalPredictors) { #Categorical predictor
      house_data[[name]] = factor(house_data[[name]], exclude = NULL)
    } else{
      if(sum(nas) > 0) {
        house_data[[name]][nas] = mean(house_data[[name]][!nas])
      }
    }
  }
}

house_data = house_data[, !colnames(house_data) %in% RM_Names]

#Remove these predictors from categorical predictors
CategoricalPredictors = CategoricalPredictors[CategoricalPredictors %in% colnames(house_data)]
#Remove these predictors from numerical predictors
NumericPredictors = NumericPredictors[NumericPredictors %in% colnames(house_data)]
```

We need to some extra step here. The test data could contain categorical values that don't present in the training data. Thus, we need to fill the missing levels (in test data) to avoid prediction problem (function:`predict` will fail if the data contains categorical values not present by dummy variable)


####Normalize Data
According to the plots in section "Check the data", we can see distribution of `SalePrice` is not normalized (why it should be?). The variance of `SalePrice ~ GrLivArea` linear relation is also not constant. Check the summary of these 2 variables.
```{r}
summary(house_data$SalePrice)
summary(house_data$GrLivArea)
```
The range of these 2 variables are pretty large. To extend this thinking, it is very likely some variables for space size (like: GrLivArea, LotArea) may have same characteristic. We can normalize and stablize the data through log transformation. 

As the rule, we find the numerical predictors that have large range (max/min > 10) and replace it with log value.

Note: It is not recommended to change the data directly and better to use model formula instead. We do this to help our later analysis and model selection.
```{r}
Log_Names = c()
for(name in NumericPredictors) {
  if(range(house_data[[name]])[1] > 0 && 
     range(house_data[[name]])[2] / range(house_data[[name]])[1] > 10) {
    Log_Names = c(Log_Names, name)
  }
}

(Log_Names)
```
```{r}
house_data = log_data(house_data, Log_Names)
```

We then replot the data.

```{r, echo=FALSE, fig.height=8, fig.width=8}
par(mfrow=c(3,2))
plot(Log_SalePrice ~ Log_GrLivArea, data = house_data, pch  = 20, cex  = 2, col  = "dodgerblue")
hist(house_data$Log_SalePrice, col = "dodgerblue",border = "darkorange")
hist(house_data$Log_GrLivArea, col = "dodgerblue",border = "darkorange")
hist(house_data$Log_LotArea, col = "dodgerblue",border = "darkorange")
boxplot(Log_SalePrice ~ OverallQual, data = house_data, pch = 20, cex = 2, col = "darkorange", border = "dodgerblue")
boxplot(Log_SalePrice ~ Neighborhood, data = house_data, pch = 20, cex = 2, col = "darkorange", border = "dodgerblue")
```

Except plot:`Log_SalePrice ~ Neighborhood` doesn't have much change, other plots look much better.

####Combine Predictors
The original data use `FullBath` and `HalfBath` to describe the bathroom information. `HalfBath` itself dosen't have strong relation with the price. Thus, we combine these two predictors to simplify the model and describe the SalePrice better.

```{r}
#Combine FullBath and HalfBath
house_data$Bath = house_data$FullBath + house_data$HalfBath * 0.5
house_data = subset(house_data, select = -c(Id, FullBath, HalfBath))
```

##Predictor Selection

####Visualize Correlation

$## Correlation pairs plot##$
```{r, include=FALSE}
#We convert all factor variables to numeric in order to call cor()
num_house_data = house_data
for(name in colnames(num_house_data)){
  if(is.factor(num_house_data[[name]])) 
    num_house_data[[name]] = as.numeric(num_house_data[[name]])
}

# number of corr plots
house_cols = colnames(num_house_data)
x = (length(house_cols)-1)/10

for (i in 0:(x-1)){
  #Need uncomment
  #pairs(num_house_data[, c(house_cols[(10*i+1):(10*(i+1))], "Log_SalePrice")], col="dodgerblue")
}
```

$## Examine correlation between `Log_SalePrice` and numerical predictors ##$
```{r, echo=FALSE}
non_factors = c("Log_LotFrontage", "Log_LotArea", "YearBuilt", 
                "YearRemodAdd", "MasVnrArea", "BsmtFinSF1",
                "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", 
                "LowQualFinSF", "Log_FirstFlrSF",
                "Log_GrLivArea", "BsmtFullBath", "BsmtHalfBath",
                "BedroomAbvGr", "KitchenAbvGr", 
                "TotRmsAbvGrd", "Fireplaces", "GarageYrBlt",
                "GarageCars", "GarageArea", "WoodDeckSF", 
                "OpenPorchSF", "EnclosedPorch",  
                "ScreenPorch", "PoolArea", "MiscVal", "Log_SalePrice")

nf_data = data.frame(sapply(house_data[, non_factors], function(x) as.numeric(x)))
colnames(nf_data) = non_factors
nf_corr = cor(nf_data)
sort(nf_corr["Log_SalePrice",], decreasing=TRUE)
```

$## Histogram ##$

Histograms of the numerical variables that are significant.

```{r, echo=FALSE}
top_corrs = sort(nf_corr["Log_SalePrice", ], decreasing=TRUE)[sort(nf_corr["Log_SalePrice", ], decreasing=TRUE) > 0]
top_corrs_name = names(top_corrs[2:length(top_corrs)])

par(mfrow=c(4,3))
for (name in top_corrs_name[1:12]){
  hist(nf_data[,name], breaks=20, xlab=name, main="hist ", col="dodgerblue")
}

par(mfrow=c(4,3))
for (name in top_corrs_name[13:length(top_corrs_name)]){
  hist(nf_data[,name], breaks=20, xlab=name, main="hist ", col="dodgerblue")
}
```

$## SalePrice vs Top Correlated Features ##$
```{r, echo=FALSE}
par(mfrow=c(3,3))
for (name in top_corrs_name[1:9]){
  plot(nf_data[,name], house_data$Log_SalePrice, xlab=name, ylab="SalePrice", col="blue")
  p_mod = lm(house_data$Log_SalePrice ~ nf_data[,name])
  abline(coef(p_mod)[1], coef(p_mod)[2], col="orange", lwd=2)
}

par(mfrow=c(3,3))
for (name in top_corrs_name[10:18]){
  plot(nf_data[,name], house_data$Log_SalePrice, xlab=name, ylab="SalePrice", col="blue")
  p_mod = lm(house_data$Log_SalePrice ~ nf_data[,name])
  abline(coef(p_mod)[1], coef(p_mod)[2], col="orange", lwd=2)
}

par(mfrow=c(3,3))
for (name in top_corrs_name[19:length(top_corrs_name)]){
  plot(nf_data[,name], house_data$Log_SalePrice, xlab=name, ylab="SalePrice", col="blue")
  p_mod = lm(house_data$Log_SalePrice ~ nf_data[,name])
  abline(coef(p_mod)[1], coef(p_mod)[2], col="orange", lwd=2)
}
```

####Store Significant Predictors
As we have some basic understanding on our data correlation, we select the significant predictors that have high correlation with SalePrice and store these predictor names to `Start_Pre_Names`.

```{r, echo=FALSE}
all_cor = cor(num_house_data)
(sig_cor = sort(abs(all_cor["Log_SalePrice", abs(all_cor["Log_SalePrice",]) > 0.5]), decreasing = TRUE)[-1])

Start_Pre_Names = names(sig_cor)
```

We first build a model with predictors that has cor values > 0.5.

```{r, warning=FALSE}
library(faraway)
sig_model = lm(build_formula("Log_SalePrice", Start_Pre_Names), data = house_data)
summary(sig_model)$adj.r.squared
```
This model has good Adjusted $R^2$ value. 

####Solve Collinearity Problem
```{r, message=TRUE, warning=FALSE}
sort(vif(sig_model)[vif(sig_model) > 5], decreasing = TRUE)
```
We see many predictors has high VIFs. Although high VIFs won't cause big problem to prediction, too many correlated predictors do impact our capability to search through other better models using interaction or polynomial. Strong colliearity could also cause problem to analyze high leverage and calculate LOOCV RMSE because not able to solve $\left(X^\top X\right)^{-1}$.

We see `OverallQual` gives us the most trouble although VIF of `OverallQual2` is `r vif(sig_model)["OverallQual2"][[1]]`. Let's first try to remove this predictor and see what will happen.
```{r}
var_names = names(sig_cor)[names(sig_cor) != "OverallQual"]
no_overallqual_model = lm(build_formula("Log_SalePrice", var_names), 
                          data = house_data)
summary(no_overallqual_model)$adj.r.squared
```
However, after removing `OverallQual`, Adjusted $R^2$ dropped `r summary(sig_model)$adj.r.squared - summary(no_overallqual_model)$adj.r.squared`. Intuitively, `OverallQual` is important to determine house price. Thus, we will try to keep `OverallQual` predictor.

If we check the `Log_SalePrice ~ OverallQual` plot above, we can see each value of `OverallQual` and the mapped mean of `Log_SalePrice` is quite linear. This means, increasing 1 score of `OverallQual` has constant change on `Log_SalePrice`. Therefore, we can convert this categorical predictor to numerical predictor.

```{r}
house_data$OverallQual = as.numeric(house_data$OverallQual)

#Update predictor names info
CategoricalPredictors = CategoricalPredictors[CategoricalPredictors != "OverallQual"]
NumericPredictors     = c("OverallQual", NumericPredictors)

model_1 = lm(build_formula("Log_SalePrice", Start_Pre_Names), data = house_data)
summary(model_1)$adj.r.squared
```

To handle the remaining predictors, we will use two approaches and compare the resulting models
- Remove VIF predictors one-by-one until the model has no collinearity problem.
- Use Exhaustive Search to find a good model and then remove high VIF predictors to solve collinearity problem.

We will make a function so that we can remove the predictor one-by-one. The reason we do this one-by-one instead of removing all high VIF predictors at once is because removing of one high VIF predictor could lower the VIF of other predictors. We try to keep as many predictors as we can in order to have good prediction.

```{r, warning=FALSE}
optimize_vif = function(y_name, x_names, data, must_have = c(), nstep = 0) {
  repeat {
    m = lm(build_formula(y_name, x_names), data = data)
    high_vifs = vif(m)[vif(m) > 5 & !names(vif(m)) %in% must_have]
    if(length(high_vifs) == 0) break

    #print(sort(high_vifs, decreasing = TRUE)[1])
    rm_name = get_predictor_name(colnames(data), 
                                 names(sort(high_vifs, decreasing = TRUE))[1])
    x_names = x_names[x_names != rm_name]
    
    nstep = nstep - 1
    if(nstep == 0) break
  }
  x_names
}
```

Firstly, let's try to call the function:`optimize_vif` to remove all high VIF variable. 
```{r, warning=FALSE}
var_names = optimize_vif("Log_SalePrice", Start_Pre_Names, 
                              data = house_data, must_have = c("OverallQual", "Log_GrLivArea"))
good_vif_model_1 = lm(build_formula("Log_SalePrice", var_names), data = house_data)
vif(good_vif_model_1)
summary(good_vif_model_1)$r.squared
```

Secondly, we do Exhaustive Search and then call function:`optimize_vif` to give us a model that is collinearity issue free.

Notice we can't do such search starting with all predictors due to high collinearity issue among these predictors (regsubsets show warning). Therefore, to start, we call function:`optimize_vif` to remove 2 highest VIF predictors and display them. 
```{r, warning=FALSE}
var_names = optimize_vif("Log_SalePrice", Start_Pre_Names, data = house_data, must_have = c("OverallQual", "Log_GrLivArea"), nstep = 2)
Start_Pre_Names[!Start_Pre_Names %in% var_names]
```
For the remaning variables, we do an exhausted search. 
```{r}
library(leaps)
all_mod = summary(regsubsets(build_formula("Log_SalePrice", var_names), data = house_data, nvmax = 20))
all_mod$adjr2
```

We choose 13th model to balance between predictor numbers and Adjusted $R^2$. We then remove all high VIF predictors. This give us a model that has no collinearity problem and has reasonable prediction coverage.
```{r}
var_names = names(all_mod$which[13,])[all_mod$which[13,]][-1]
var_names = get_predictor_name(colnames(house_data), var_names)
var_names = optimize_vif("Log_SalePrice", var_names, data = house_data, 
                           must_have = c("OverallQual", "Log_GrLivArea"))
good_vif_model_2 = lm(build_formula("Log_SalePrice", var_names), data = house_data)
vif(good_vif_model_2)
summary(good_vif_model_2)$r.squared
```

Since the second model has much less predictors, small and similar Adjusted $R^2$, we will choose this model and assign it to **`good_vif_model`** for future usage.
```{r}
good_vif_model = good_vif_model_2
```

Note: anova test between this two models also gives p-value of `r anova(good_vif_model_1, good_vif_model_2)[2, "Pr(>F)"]` which is not extremely small. However, since this model dosen't comply with linear assumption, we can't use this value to make the decision. 

## Optimize for LOOCV RMSE 
Until now, we focus on finding model which is small and collinearity problem free. Since our goal is prediction, we now swith to find a model that has lowest LOOC RMSE and start from  `good_vif_model`.

Let's first check its LOOCV RMSE.
```{r}
get_loocv_rmse(good_vif_model)
```

To improve LOOCV RMSE and fit the data better, intuitively we would add more predictors to the model. Noticed the model only has one categorical predictor `GarageFinish`, we probably can try to add some categorical predictors we think is important to this model. For example: Neighborhood.
```{r}
model_try = lm(build_formula("Log_SalePrice", c(Start_Pre_Names, "Neighborhood")), data = house_data)
get_loocv_rmse(model_try)
```
The LOOCV RMSE dose improved. This gives us some confidence that we can find a model with much better/smaller LOOCV RMSE.

Instead of trying to add/remove predictors one-by-one manually, inspired by R `step()` function, we build a similar function to explore and find the model with lowest LOOCV RMSE.
```{r}
step_loocv_rmse =  function(start_model, extra_predictors, data, trace = 1) {
  min_rmse     = get_loocv_rmse(start_model)
  res_var_name = names(attr(summary(start_model)$term,"dataClasses")[1])
  
  pre_names    = attr(summary(start_model)$term,"term.labels")
  
  while(length(extra_predictors) > 0) {
    a_rmse = rep(Inf, length(extra_predictors))
    for(i in 1: length(extra_predictors)) {
      model = lm(build_formula(res_var_name, c(pre_names, extra_predictors[i])), 
                 data = data)
      
      if (model$rank == ncol(model.matrix(model))){
        #We only consider predictor that won't cause sigularity issue
        a_rmse[i] = get_loocv_rmse(model)
      }
    }
    
    rmse_i = which.min(a_rmse)
    if(min_rmse <= a_rmse[rmse_i]) {
      break
    }
    if(trace == 1) print(paste("+", extra_predictors[rmse_i], ", LOOCV RMSE:", 
                               a_rmse[rmse_i], sep = ""))
    
    pre_names        = c(pre_names, extra_predictors[rmse_i])
    extra_predictors = extra_predictors[!extra_predictors == extra_predictors[rmse_i]]
    
    min_rmse         = a_rmse[rmse_i]
  }
  lm(build_formula(res_var_name, pre_names), data = data)
}
```

Then we can use this function to search model that has best LOOCV RMSE. We try all predictors not in the model `good_vif_model`.
```{r}
extra_predictors = colnames(house_data)[!colnames(house_data) %in% 
                                          c(names(coef(good_vif_model)), "Log_SalePrice")]

loocv_rmse_model = step_loocv_rmse(good_vif_model, extra_predictors, data = house_data)
```
```{r}
get_loocv_rmse(loocv_rmse_model)
summary(loocv_rmse_model)$adj.r.squared
```
**Our function find us a way better model interm of LOOCV RMSE!**

The predictors are:
`r attr(summary(loocv_rmse_model)$term, "term.labels")`

Let's try to do forward search using AIC/BIC and see whether we can find better ones.
```{r}
pre_names         = colnames(house_data)[!colnames(house_data) %in% c("Log_SalePrice")]
forward_aic_model = step(good_vif_model, scope = build_formula("Log_SalePrice", pre_names),
                         direction = "forward", trace = 0)
forward_bic_model = step(good_vif_model, scope = build_formula("Log_SalePrice", pre_names),
                         direction = "forward", trace = 0, k = log(nrow(house_data)))

summary(forward_aic_model)$adj.r.squared
get_loocv_rmse(forward_aic_model)
summary(forward_bic_model)$adj.r.squared
get_loocv_rmse(forward_bic_model)

#forward_aic_model = loocv_rmse_model
#forward_bic_model = loocv_rmse_model
```
These 2 models do have better Adjusted $R^2$. However, they cant calculate LOOCV RSME due to strong collinearity between predictors. We save them to compare prediction results.

Let's try something crazy. Do interaction between all categorical and numberic predictors in `loocv_rmse_model`.
```{r}
names     = attr(summary(loocv_rmse_model)$term, "term.labels")
cat_names = CategoricalPredictors[CategoricalPredictors %in% names]
num_names = names[!names %in% cat_names]

crazy_int_model = lm(build_formula("Log_SalePrice", num_names, cat_names), 
                          data = house_data)
summary(crazy_int_model)$adj.r.squared
get_loocv_rmse(crazy_int_model)
```
This model gives us the best Adjusted $R^2$ but also no LOOCV RSME.

Lastely, we make a model using all predictors,
```{r}
full_model = lm(Log_SalePrice ~ ., data = house_data)
summary(full_model)$adj.r.squared
get_loocv_rmse(full_model)
```

#Results 
##Test Models 
To verify each modelʻs predictive power, we split the data set into a training and test set and re-train the model on the training data.

$## Split Model into Train & Test Sets ##$

```{r}
set.seed(42)
train_index = sample(1:nrow(house_data), nrow(house_data)*2/3) # randomly chosen observations for training
train_data  = house_data[train_index, ]
test_data   = house_data[-train_index, ]
```

```{r}
## ADD MORE MODELS HERE TO COMPARE

#"Full", "ForwardAIC"

modelNames = c("BestVIF", "BestLOOCV_RMSE", "CrazyInteraction", "Full", "ForwardAIC", "ForwardBIC")
models = vector(mode="list", length = length(modelNames))

# retrain
# models[[1]] = lm(build_formula(summary(good_vif_model)$term[[2]],
#                            attr(summary(good_vif_model)$term,"term.labels")), data = train_data)
# models[[2]] = lm(build_formula(summary(loocv_rmse_model)$term[[2]],
#                            attr(summary(loocv_rmse_model)$term,"term.labels")), data = train_data)
# models[[3]] = lm(build_formula(summary(crazy_int_model)$term[[2]],
#                            attr(summary(crazy_int_model)$term,"term.labels")), data = train_data)
# models[[4]] = lm(build_formula(summary(full_model)$term[[2]],
#                            attr(summary(full_model)$term,"term.labels")), data = train_data)
# models[[5]] = lm(build_formula(summary(forward_aic_model)$term[[2]],
#                            attr(summary(forward_aic_model)$term,"term.labels")), data = train_data)
# models[[4]] = lm(build_formula(summary(forward_bic_model)$term[[2]],
#                            attr(summary(forward_bic_model)$term,"term.labels")), data = train_data)

#compareModels(models, modelNames)
```

The prediction errors for each model were also very similar.
```{r}
## ADD MORE MODELS HERE TO COMPARE
par(mfrow=c(2, 2))
# for(i in 1:length(modelNames)) plot_errors(test_data, models[i], modelNames[i])
```

We summarize 5 models: `full_model`, `loocv_rmse_model`, `loocv_rmse_model_int`, `forward_aic_model` and `forward_bic_model` as following.

|              |Full      | Best LOOCV RSME | Interactive LOOCV RSME | Forward AIC | Forward BIC |
|--------------|----------|-----------------|------------------------|-------------|-------------|
|Adjusted $R^2$|`r summary(full_model)$adj.r.squared`|`r summary(loocv_rmse_model)$adj.r.squared`|`r summary(crazy_int_model)$adj.r.squared`|`r summary(forward_aic_model)$adj.r.squared`|`r summary(forward_bic_model)$adj.r.squared`|
|AIC           |`r AIC(full_model)`|`r AIC(loocv_rmse_model)`|`r AIC(crazy_int_model)`|`r AIC(forward_aic_model)`|`r AIC(forward_bic_model)`|
|BIC           |`r BIC(full_model)`|`r BIC(loocv_rmse_model)`|`r BIC(crazy_int_model)`|`r BIC(forward_aic_model)`|`r BIC(forward_bic_model)`|
|LOOCV RSME    | N/A | `r get_loocv_rmse(loocv_rmse_model)` | N/A | N/A | N/A |

Each model scores high on some measurements but low on others. We will use these models to make prediction on [test.csv](file:HousePrices/test.csv) and get [Kaggle score](https://www.kaggle.com/c/house-prices-advanced-regression-techniques#evaluation) to check their performance.

The score are calculated on RMSE between the logarithm of the predicted value and the logarithm of the observed sales price.

To do: influence analysis, outlier analysis


We load data, store it to `test_data` and process the data as we did to the training data.
```{r, message=FALSE, include=FALSE}
test_data = read_csv("HousePrices/test.csv")

#Remove predictors removed in house_data
test_data = test_data[, !colnames(test_data) %in% RM_Names]

#Convert to categorial predictor to categorical data type
for(name in colnames(test_data)) {
  if(name %in% CategoricalPredictors) {
    test_data[[name]] = factor(test_data[[name]], exclude = NULL)
  } else {
    nas = is.na(test_data[[name]])
    test_data[[name]][nas] = mean(test_data[[name]][!nas])
  }
}

colnames(test_data) = replace_names(test_data, BadNames)
test_data$Bath = test_data$FullBath + test_data$HalfBath * 0.5
test_data = log_data(test_data, Log_Names[Log_Names != "SalePrice"])
```


```{r, eval=FALSE, include=FALSE}
make_prediction = function(model, data) {
  data.frame(id = data$Id, SalePrice = exp(predict(model, newdata = data)))
}

write.csv(make_prediction(good_vif_model, test_data), "HousePrices/Submission/good_vif.csv", row.names = FALSE)
write.csv(make_prediction(loocv_rmse_model, test_data), "HousePrices/Submission/loocv_rmse.csv", row.names = FALSE)
write.csv(make_prediction(crazy_int_model, test_data), "HousePrices/Submission/crazy_int.csv", row.names = FALSE)
write.csv(make_prediction(full_model, test_data), "HousePrices/Submission/full.csv", row.names = FALSE)
write.csv(make_prediction(forward_aic_model, test_data), "HousePrices/Submission/forward_aic.csv", row.names = FALSE)
write.csv(make_prediction(forward_bic_model, test_data), "HousePrices/Submission/forward_bic.csv", row.names = FALSE)

```
![Kaggle Score Screen Shot](Kaggle.png)

#Discussion

#### Improvement

#### N-Fold Cross-Validation

#### Model Assumption Check

#### Outlier & Influence Analysis

#Appendix
- [Kaggle Competition - House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
- Books
    - [Applied Statistics with R by David Dalpiaz](http://daviddalpiaz.github.io/appliedstats/)
    - [An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370)
- [Article: Examining your data](http://www.personal.psu.edu/jxb14/M554/articles/Hair%20et%20al%202010%20--%20Chapter%202.pdf)
- [Data Description](file:HousePrices/data_description.txt)
- [Utility Functions](file:util.R)